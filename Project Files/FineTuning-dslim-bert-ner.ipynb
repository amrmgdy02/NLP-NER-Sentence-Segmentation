{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-19T10:36:32.182710Z",
     "iopub.status.busy": "2024-12-19T10:36:32.182405Z",
     "iopub.status.idle": "2024-12-19T10:36:57.822549Z",
     "shell.execute_reply": "2024-12-19T10:36:57.821338Z",
     "shell.execute_reply.started": "2024-12-19T10:36:32.182660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import torch\n",
    "import json\n",
    "import pipeline_utils as utils\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/kaggle/input/pizza-train/PIZZA_train.json\"\n",
    "\n",
    "# Read and preprocess the dataset\n",
    "data = []\n",
    "with open(dataset_path, \"r\") as f: # Added encoding='utf-8'\n",
    "    for line in f:\n",
    "        entry = json.loads(line.strip())\n",
    "        top = entry[\"train.TOP\"]\n",
    "        data.append(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:36:57.823973Z",
     "iopub.status.busy": "2024-12-19T10:36:57.823527Z",
     "iopub.status.idle": "2024-12-19T10:36:58.423296Z",
     "shell.execute_reply": "2024-12-19T10:36:58.422366Z",
     "shell.execute_reply.started": "2024-12-19T10:36:57.823941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train.TOP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ORDER can i have (PIZZAORDER (NUMBER a ) (SIZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(ORDER (PIZZAORDER (SIZE large ) pie with (TOP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(ORDER (PIZZAORDER (SIZE party size ) (STYLE s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(ORDER can i have (PIZZAORDER (NUMBER one ) (S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           train.TOP\n",
       "0  (ORDER can i have (PIZZAORDER (NUMBER a ) (SIZ...\n",
       "1  (ORDER (PIZZAORDER (SIZE large ) pie with (TOP...\n",
       "2  (ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE ...\n",
       "3  (ORDER (PIZZAORDER (SIZE party size ) (STYLE s...\n",
       "4  (ORDER can i have (PIZZAORDER (NUMBER one ) (S..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: create dataframe of train.TOP\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data, columns=['train.TOP'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:36:58.425051Z",
     "iopub.status.busy": "2024-12-19T10:36:58.424641Z",
     "iopub.status.idle": "2024-12-19T10:37:04.710808Z",
     "shell.execute_reply": "2024-12-19T10:37:04.709864Z",
     "shell.execute_reply.started": "2024-12-19T10:36:58.425028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of orders with pizza and drink 341942\n",
      "Num of orders with only drink 425116\n",
      "Num of orders with only pizza 1689388\n"
     ]
    }
   ],
   "source": [
    "# DRINK WITH PIZZA\n",
    "pizza_drink_df = df[df['train.TOP'].str.contains('PIZZAORDER') & df['train.TOP'].str.contains('DRINKORDER')]\n",
    "num_pizza_drink_rows = len(pizza_drink_df)\n",
    "print(\"Num of orders with pizza and drink\", num_pizza_drink_rows)\n",
    "pizza_drink_df = pizza_drink_df.sample(20000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# DRINK\n",
    "drink_df = df[df['train.TOP'].str.contains('DRINKORDER') & ~df['train.TOP'].str.contains('PIZZAORDER')]\n",
    "num_drink_rows = len(drink_df)\n",
    "print(\"Num of orders with only drink\", num_drink_rows)\n",
    "drink_df = drink_df.sample(20000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# PIZZA\n",
    "pizza_df = df[df['train.TOP'].str.contains('PIZZAORDER') & ~df['train.TOP'].str.contains('DRINKORDER')]\n",
    "num_pizza_rows = len(pizza_df)\n",
    "print(\"Num of orders with only pizza\", num_pizza_rows)\n",
    "pizza_df = pizza_df.sample(30000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df = pd.concat([pizza_df, drink_df, pizza_drink_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:37:04.898306Z",
     "iopub.status.busy": "2024-12-19T10:37:04.898108Z",
     "iopub.status.idle": "2024-12-19T10:37:16.426108Z",
     "shell.execute_reply": "2024-12-19T10:37:16.425390Z",
     "shell.execute_reply.started": "2024-12-19T10:37:04.898289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, f_type):\n",
    "        df[\"IS\"] = df[f\"{f_type}.TOP\"].apply(utils.tag_orders)\n",
    "        df[\"IS.tokens\"] = df[\"IS\"].apply(lambda x: x[0])\n",
    "        df[\"IS.tag_ids\"] = df[\"IS\"].apply(lambda x: x[1])\n",
    "        df[\"IS.tags\"] = df[\"IS\"].apply(lambda x: x[2])\n",
    "        df = df.drop(columns=[\"IS\"])\n",
    "        df['grouped_tokens'] = df.apply(lambda row: utils.group_order_tokens(row['IS.tokens'], row['IS.tags']), axis=1)\n",
    "        df['NER.tag_ids'] = df.apply(lambda row: utils.parse_top_string(row[f\"{f_type}.TOP\"], utils.entity_patterns, utils.tag2id)[1],\n",
    "                                        axis=1)\n",
    "        df[\"NER.tags\"] = df[\"NER.tag_ids\"].apply(lambda x: [utils.id2tag[tag_id] for tag_id in x])\n",
    "        df['grouped_ids'] = df.apply(\n",
    "                lambda row: utils.group_corresponding_tags(row['grouped_tokens'], row['IS.tokens'], row['NER.tag_ids']), axis=1)\n",
    "        # downcase the tokens\n",
    "        df['IS.tokens'] = df['IS.tokens'].apply(lambda row: [word.lower() for word in row])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_dataframe(df, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:38:38.413874Z",
     "iopub.status.busy": "2024-12-19T10:38:38.413531Z",
     "iopub.status.idle": "2024-12-19T10:38:38.449872Z",
     "shell.execute_reply": "2024-12-19T10:38:38.449137Z",
     "shell.execute_reply.started": "2024-12-19T10:38:38.413846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the first DataFrame\n",
    "IS_df = df[['IS.tokens', 'IS.tag_ids']].copy().rename(columns={'IS.tokens': 'tokens', 'IS.tag_ids': 'ner_tags'})\n",
    "\n",
    "# Create the second DataFrame\n",
    "NER_df = df[['IS.tokens', 'NER.tag_ids']].copy().rename(columns={'IS.tokens': 'tokens', 'NER.tag_ids': 'ner_tags'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:38:39.676043Z",
     "iopub.status.busy": "2024-12-19T10:38:39.675741Z",
     "iopub.status.idle": "2024-12-19T10:38:39.681253Z",
     "shell.execute_reply": "2024-12-19T10:38:39.680239Z",
     "shell.execute_reply.started": "2024-12-19T10:38:39.676021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\ndef split_multiple_orders(df):\\n    expanded_tokens = []\\n    expanded_tags = []\\n    \\n    # Iterate through each row\\n    for _, row in df.iterrows():\\n        tokens = row[\\'tokens\\']  # List of lists of tokens\\n        ner_tags = row[\\'ner_tags\\']  # List of lists of tags\\n        \\n        assert len(tokens) == len(ner_tags), \"Tokens and tags must have same length\"\\n        \\n        # Add each order as a separate row\\n        for single_order_tokens, single_order_tags in zip(tokens, ner_tags):\\n            expanded_tokens.append(single_order_tokens)  # Wrap in list to maintain structure\\n            expanded_tags.append(single_order_tags)  # Wrap in list to maintain structure\\n    \\n    expanded_df = pd.DataFrame({\\n        \\'tokens\\': expanded_tokens,\\n        \\'ner_tags\\': expanded_tags\\n    })\\n    \\n    return expanded_df\\n\\nNER_df = split_multiple_orders(NER_df)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_multiple_orders(df):\n",
    "    expanded_tokens = []\n",
    "    expanded_tags = []\n",
    "    \n",
    "    # Iterate through each row\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = row['tokens']  # List of lists of tokens\n",
    "        ner_tags = row['ner_tags']  # List of lists of tags\n",
    "        \n",
    "        assert len(tokens) == len(ner_tags), \"Tokens and tags must have same length\"\n",
    "        \n",
    "        # Add each order as a separate row\n",
    "        for single_order_tokens, single_order_tags in zip(tokens, ner_tags):\n",
    "            expanded_tokens.append(single_order_tokens)  # Wrap in list to maintain structure\n",
    "            expanded_tags.append(single_order_tags)  # Wrap in list to maintain structure\n",
    "    \n",
    "    expanded_df = pd.DataFrame({\n",
    "        'tokens': expanded_tokens,\n",
    "        'ner_tags': expanded_tags\n",
    "    })\n",
    "    \n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df = split_multiple_orders(NER_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:38:43.611337Z",
     "iopub.status.busy": "2024-12-19T10:38:43.611034Z",
     "iopub.status.idle": "2024-12-19T10:38:45.164397Z",
     "shell.execute_reply": "2024-12-19T10:38:45.163732Z",
     "shell.execute_reply.started": "2024-12-19T10:38:43.611314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "IS_dataset = Dataset.from_pandas(IS_df)\n",
    "NER_dataset = Dataset.from_pandas(NER_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:38:45.803635Z",
     "iopub.status.busy": "2024-12-19T10:38:45.803191Z",
     "iopub.status.idle": "2024-12-19T10:39:33.794829Z",
     "shell.execute_reply": "2024-12-19T10:39:33.794091Z",
     "shell.execute_reply.started": "2024-12-19T10:38:45.803608Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a3eeb3bdc6435386c924b15e576c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec666e23386f49a5a144a0fd4707346c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2870309269a2412190ae780831b96198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf28c3fcdd1c44459f3b723e7e33f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc331ccb9b74ba0835b2cfb896e552d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0ff7d47b1b42aa970fffe0ff8434fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/70000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbd584a568e477cb481792901c75e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/70000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def create_ner_preprocessing_pipeline(model_name='dslim/bert-base-NER', max_length=55):\n",
    "    \"\"\"\n",
    "    Creates a complete preprocessing pipeline for NER tasks.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the pre-trained model to use\n",
    "        max_length (int): Maximum sequence length for tokenization\n",
    "        \n",
    "    Returns:\n",
    "        tokenizer: The loaded tokenizer\n",
    "        preprocess_function: The preprocessing function\n",
    "        data_collator: The data collator for token classification\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def tokenize_and_align_labels(examples):\n",
    "        \"\"\"\n",
    "        Tokenize inputs and align labels for NER tasks.\n",
    "        \n",
    "        Args:\n",
    "            examples (dict): Dictionary containing 'tokens' and 'ner_tags'\n",
    "            \n",
    "        Returns:\n",
    "            dict: Processed features with aligned labels\n",
    "        \"\"\"\n",
    "        if 'tokens' not in examples or 'ner_tags' not in examples:\n",
    "            raise ValueError(\"Examples must be a dictionary with 'tokens' and 'ner_tags' keys\")\n",
    "            \n",
    "        if len(examples['tokens']) != len(examples['ner_tags']):\n",
    "            raise ValueError(\"Number of token sequences doesn't match number of label sequences\")\n",
    "            \n",
    "        # Tokenize the input tokens\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"tokens\"],\n",
    "            truncation=True,\n",
    "            is_split_into_words=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=None  # Return lists instead of tensors\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            #previous_word_id = None\n",
    "            aligned_labels = []\n",
    "        \n",
    "            # Validate label sequence length\n",
    "            if len(label) != len(examples[\"tokens\"][i]):\n",
    "                raise ValueError(f\"Mismatch between tokens and labels at index {i}\")\n",
    "           \n",
    "            for word_id in word_ids:\n",
    "                if word_id is None:\n",
    "                    # Special tokens get labeled as -100\n",
    "                    aligned_labels.append(-100)\n",
    "                else:\n",
    "                    try:\n",
    "                        aligned_labels.append(label[word_id])\n",
    "                    except IndexError:\n",
    "                        raise IndexError(f\"Label index {word_id} out of range for sequence {i}\")\n",
    "                #else:\n",
    "                #    # Subsequent subword tokens get labeled as -100\n",
    "                 #   aligned_labels.append(-100)\n",
    "                #previous_word_id = word_id\n",
    "            #print(aligned_labels)\n",
    "            labels.append(aligned_labels)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        \n",
    "        # Verify lengths match\n",
    "        for key in tokenized_inputs.keys():\n",
    "            if len(tokenized_inputs[key]) != len(examples['tokens']):\n",
    "                raise ValueError(f\"Length mismatch in processed features for key: {key}\")\n",
    "        \n",
    "        return tokenized_inputs\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        pad_to_multiple_of=8  # Helpful for hardware optimization\n",
    "    )\n",
    "    \n",
    "    return tokenizer, tokenize_and_align_labels, data_collator\n",
    "\n",
    "def process_dataset(dataset, preprocessing_pipeline):\n",
    "    \"\"\"\n",
    "    Process a dataset using the preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The input dataset (must have 'tokens' and 'ner_tags' columns)\n",
    "        preprocessing_pipeline: The preprocessing function from create_ner_preprocessing_pipeline\n",
    "        \n",
    "    Returns:\n",
    "        processed_dataset: The processed dataset ready for training\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processed_dataset = dataset.map(\n",
    "            preprocessing_pipeline,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Processing dataset\"\n",
    "        )\n",
    "        return processed_dataset\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing dataset: {str(e)}\")\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "IS_tokenizer, IS_preprocess_fn, IS_data_collator = create_ner_preprocessing_pipeline()\n",
    "NER_tokenizer, NERpreprocess_fn, NER_data_collator = create_ner_preprocessing_pipeline()\n",
    "\n",
    "processed_IS_dataset = process_dataset(IS_dataset, IS_preprocess_fn)\n",
    "processed_NER_dataset = process_dataset(NER_dataset, NERpreprocess_fn)\n",
    "\n",
    "IS_batch = IS_data_collator([processed_IS_dataset[i] for i in range(len(processed_IS_dataset))])\n",
    "NER_batch = NER_data_collator([processed_NER_dataset[i] for i in range(len(processed_NER_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:39:39.418651Z",
     "iopub.status.busy": "2024-12-19T10:39:39.417829Z",
     "iopub.status.idle": "2024-12-19T10:39:40.691004Z",
     "shell.execute_reply": "2024-12-19T10:39:40.690415Z",
     "shell.execute_reply.started": "2024-12-19T10:39:39.418606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:39:51.409441Z",
     "iopub.status.busy": "2024-12-19T10:39:51.408778Z",
     "iopub.status.idle": "2024-12-19T10:39:55.605007Z",
     "shell.execute_reply": "2024-12-19T10:39:55.604111Z",
     "shell.execute_reply.started": "2024-12-19T10:39:51.409413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcecbf93b0494208b53fc5fcd845e3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([21]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([21, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "IS_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'dslim/bert-base-NER',\n",
    "    num_labels=len(utils.IS_tag2id),\n",
    "    label2id=utils.IS_tag2id,\n",
    "    id2label=utils.IS_id2tag,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "NER_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'dslim/bert-base-NER',\n",
    "    num_labels=len(utils.tag2id),\n",
    "    label2id=utils.tag2id,\n",
    "    id2label=utils.id2tag,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "IS_training_args = TrainingArguments(\n",
    "    output_dir=\"./pizza_is_model\",   # Directory to save the model\n",
    "    learning_rate=2e-5,               # Standard BERT fine-tuning LR\n",
    "    per_device_train_batch_size=128,   # Batch size\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=5,               # Number of epochs\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",            # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,      # Load the best model at the end\n",
    "    metric_for_best_model=\"f1\",       # Use F1 as the evaluation metric\n",
    ")\n",
    "\n",
    "NER_training_args = TrainingArguments(\n",
    "    output_dir=\"./pizza_ner_model\",   \n",
    "    learning_rate=2e-5,             \n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=5,             \n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",        \n",
    "    load_best_model_at_end=True,   \n",
    "    metric_for_best_model=\"f1\",    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:40:00.289611Z",
     "iopub.status.busy": "2024-12-19T10:40:00.289313Z",
     "iopub.status.idle": "2024-12-19T10:40:00.295329Z",
     "shell.execute_reply": "2024-12-19T10:40:00.294137Z",
     "shell.execute_reply.started": "2024-12-19T10:40:00.289584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# 3. Define metrics calculation\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred.predictions, pred.label_ids\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (-100)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        np.concatenate(true_labels),\n",
    "        np.concatenate(true_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:40:01.653989Z",
     "iopub.status.busy": "2024-12-19T10:40:01.653554Z",
     "iopub.status.idle": "2024-12-19T10:40:01.716599Z",
     "shell.execute_reply": "2024-12-19T10:40:01.715721Z",
     "shell.execute_reply.started": "2024-12-19T10:40:01.653952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "split_IS_dataset = processed_IS_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "split_NER_dataset = processed_NER_dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:40:06.203925Z",
     "iopub.status.busy": "2024-12-19T10:40:06.203566Z",
     "iopub.status.idle": "2024-12-19T10:40:06.815413Z",
     "shell.execute_reply": "2024-12-19T10:40:06.814295Z",
     "shell.execute_reply.started": "2024-12-19T10:40:06.203894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IS_trainer = Trainer(\n",
    "    model=IS_model,\n",
    "    args=IS_training_args,\n",
    "    train_dataset=split_IS_dataset[\"train\"],\n",
    "    eval_dataset=split_IS_dataset[\"test\"],\n",
    "    data_collator=IS_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "NER_trainer = Trainer(\n",
    "    model=NER_model,\n",
    "    args=NER_training_args,\n",
    "    train_dataset=split_NER_dataset[\"train\"],\n",
    "    eval_dataset=split_NER_dataset[\"test\"],\n",
    "    data_collator=NER_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:40:08.307015Z",
     "iopub.status.busy": "2024-12-19T10:40:08.306661Z",
     "iopub.status.idle": "2024-12-19T10:59:06.289667Z",
     "shell.execute_reply": "2024-12-19T10:59:06.288847Z",
     "shell.execute_reply.started": "2024-12-19T10:40:08.306985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2190' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2190/2190 18:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.999762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.999813</td>\n",
       "      <td>0.999813</td>\n",
       "      <td>0.999813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.999860</td>\n",
       "      <td>0.999860</td>\n",
       "      <td>0.999860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.999903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.999910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2190, training_loss=0.026670368056591242, metrics={'train_runtime': 1136.404, 'train_samples_per_second': 246.391, 'train_steps_per_second': 1.927, 'total_flos': 5716848412800000.0, 'train_loss': 0.026670368056591242, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T10:59:06.291182Z",
     "iopub.status.busy": "2024-12-19T10:59:06.290943Z",
     "iopub.status.idle": "2024-12-19T11:18:06.860905Z",
     "shell.execute_reply": "2024-12-19T11:18:06.860034Z",
     "shell.execute_reply.started": "2024-12-19T10:59:06.291160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2190' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2190/2190 18:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.999984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2190, training_loss=0.008094513797749803, metrics={'train_runtime': 1139.797, 'train_samples_per_second': 245.658, 'train_steps_per_second': 1.921, 'total_flos': 5716021584000000.0, 'train_loss': 0.008094513797749803, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IS_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:18:06.862447Z",
     "iopub.status.busy": "2024-12-19T11:18:06.862161Z",
     "iopub.status.idle": "2024-12-19T11:18:06.867445Z",
     "shell.execute_reply": "2024-12-19T11:18:06.866618Z",
     "shell.execute_reply.started": "2024-12-19T11:18:06.862424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /kaggle/working/fine_tuned_pizza_ner_10_10_17K created!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ner_model_dir = '/kaggle/working/fine_tuned_pizza_ner_10_10_17K'\n",
    "os.makedirs(ner_model_dir, exist_ok=True)\n",
    "print(f\"Directory {ner_model_dir} created!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T11:23:19.172328Z",
     "iopub.status.busy": "2024-12-19T11:23:19.172029Z",
     "iopub.status.idle": "2024-12-19T11:23:20.186470Z",
     "shell.execute_reply": "2024-12-19T11:23:20.185770Z",
     "shell.execute_reply.started": "2024-12-19T11:23:19.172308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/fine_tuned_pizza_ner_10_10_17K/tokenizer_config.json',\n",
       " '/kaggle/working/fine_tuned_pizza_ner_10_10_17K/special_tokens_map.json',\n",
       " '/kaggle/working/fine_tuned_pizza_ner_10_10_17K/vocab.txt',\n",
       " '/kaggle/working/fine_tuned_pizza_ner_10_10_17K/added_tokens.json',\n",
       " '/kaggle/working/fine_tuned_pizza_ner_10_10_17K/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NER_model.save_pretrained(ner_model_dir)\n",
    "NER_tokenizer.save_pretrained(ner_model_dir)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6198912,
     "sourceId": 10229558,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
